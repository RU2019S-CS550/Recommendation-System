{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def getUIdx(row):\n",
    "\tuIdx, mIdx, rate, time = row\n",
    "\treturn (uIdx, 1)\n",
    "\n",
    "def getMIdx(row):\n",
    "\tuIdx, mIdx, rate, time = row\n",
    "\treturn (mIdx, 1)\n",
    "\n",
    "def count(x, y):\n",
    "\treturn x+y\n",
    "\n",
    "class splitData(object):\n",
    "\tdef __init__(self, uList, mList):\n",
    "\t\tself.uSet = frozenset(uList)\n",
    "\t\tself.mSet = frozenset(mList)\n",
    "\t\tself.uSize = len(self.uSet)\n",
    "\t\tself.mSize = len(self.mSet)\n",
    "\t\t#(1 / 2.2) ** 2 = 0.21\n",
    "\t\tself.uTestIdx = set(random.sample(uList, int(self.uSize // 2.2)))\n",
    "\t\tself.mTestIdx = set(random.sample(mList, int(self.mSize // 2.2)))\n",
    "\t\treturn\n",
    "\n",
    "\tdef split(self, row):\n",
    "\t\tuIdx, mIdx, rate, time = row\n",
    "\t\tif uIdx in self.uTestIdx and mIdx in self.mTestIdx:\n",
    "\t\t\treturn (random.randint(1, 2), row)\n",
    "\t\telse:\n",
    "\t\t\treturn (0, row)\n",
    "\n",
    "\t#compensate trainData if trainData is not full in terms of uIdx or mIdx\n",
    "\tdef update(self, tUList, tMList):\n",
    "\t\tself.cUIdx = self.uSet - frozenset(tUList)\n",
    "\t\tself.cMIdx = self.mSet - frozenset(tMList)\n",
    "\t\treturn\n",
    "\n",
    "\tdef compensate(self, line):\n",
    "\t\tkey, row = line\n",
    "\t\tuIdx, mIdx, rate, time = row\n",
    "\t\tif uIdx in self.cUIdx or mIdx in self.cMIdx:\n",
    "\t\t\treturn (0, row)\n",
    "\t\telse:\n",
    "\t\t\treturn (key, row)\n",
    "\n",
    "def getRow(data):\n",
    "\tkey, row = data\n",
    "\treturn row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rateData = spark.read.csv('/user/hz333/data/project/ratings.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(uIdx, mIdx, rate, time) => (uIdx, 1)\n",
    "uIdx = rateData.rdd.map(getUIdx)\n",
    "#(uIdx, mIdx, rate, time) => (mIdx, 1)\n",
    "mIdx = rateData.rdd.map(getMIdx)\n",
    "\n",
    "#(uIdx, 1) => (uIdx, count)\n",
    "uIdx = uIdx.reduceByKey(count)\n",
    "#(mIdx, 1) => (mIdx, count)\n",
    "mIdx = mIdx.reduceByKey(count)\n",
    "\n",
    "#(uIdx, count) => [uIdx]\n",
    "uList = uIdx.keys().collect()\n",
    "#(mIdx, count) => [mIdx]\n",
    "mList = mIdx.keys().collect()\n",
    "\n",
    "sp = splitData(uList, mList)\n",
    "#(uIdx, mIdx, rate, time) => (key, (uIdx, mIdx, rate, time))\n",
    "data = rateData.rdd.map(sp.split)\n",
    "\n",
    "#(key, (uIdx, mIdx, rate, time)) => (0, (uIdx, mIdx, rate, time))\n",
    "trainData = data.filter(lambda line: line[0] == 0)\n",
    "#(key, (uIdx, mIdx, rate, time)) => (key, (uIdx, mIdx, rate, time))\n",
    "TVData = data.filter(lambda line: line[0] > 0)\n",
    "\n",
    "#(0, (uIdx, mIdx, rate, time)) => (uIdx, mIdx, rate, time)\n",
    "trainData = trainData.map(getRow)\n",
    "\n",
    "#get [uIdx] and [mIdx] of trainData\n",
    "tUIdx = trainData.map(getUIdx)\n",
    "tMIdx = trainData.map(getMIdx)\n",
    "\n",
    "tUIdx = tUIdx.reduceByKey(count)\n",
    "tMIdx = tMIdx.reduceByKey(count)\n",
    "\n",
    "tUList = tUIdx.keys().collect()\n",
    "tMList = tMIdx.keys().collect()\n",
    "\n",
    "\n",
    "sp.update(tUList, tMList)\n",
    "#(key, (uIdx, mIdx, rate, time)) => (newKey, (uIdx, mIdx, rate, time))\n",
    "TVData = TVData.map(sp.compensate)\n",
    "\n",
    "#union compoensated trainData\n",
    "cTrainData = TVData.filter(lambda line: line[0] == 0)\n",
    "cTrainData = cTrainData.map(getRow)\n",
    "trainData = trainData.union(cTrainData)\n",
    "\n",
    "#get testData and validData\n",
    "validData = TVData.filter(lambda line: line[0] == 2)\n",
    "testData = TVData.filter(lambda line: line[0] == 1)\n",
    "\n",
    "validData = validData.map(getRow)\n",
    "testData = testData.map(getRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check trainData.idx is full\n",
    "tUIdx = trainData.map(getUIdx)\n",
    "tMIdx = trainData.map(getMIdx)\n",
    "\n",
    "tUIdx = tUIdx.reduceByKey(count)\n",
    "tMIdx = tMIdx.reduceByKey(count)\n",
    "\n",
    "tUList = tUIdx.keys().collect()\n",
    "tMList = tMIdx.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tUList) == len(uList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tMList) == len(mList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainCSV = spark.createDataFrame(trainData, samplingRatio = 1)\n",
    "trainCSV.repartition(1).write.option('header', 'false').csv('/user/hz333/data/project/train.csv')\n",
    "testCSV = spark.createDataFrame(testData, samplingRatio = 1)\n",
    "testCSV.repartition(1).write.option('header', 'false').csv('/user/hz333/data/project/test.csv')\n",
    "validCSV = spark.createDataFrame(validData, samplingRatio = 1)\n",
    "validCSV.repartition(1).write.option('header', 'false').csv('/user/hz333/data/project/valid.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
